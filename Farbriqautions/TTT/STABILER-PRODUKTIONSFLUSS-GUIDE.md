# ğŸ”§ Stabiler Produktionsfluss - VollstÃ¤ndiger Guide

> **[.TTT T,.&T,,.T,,,.T.] TOGETHERSYSTEMS. INTERNATIONAL TTT**
> 
> Industrielle Reproduzierbarkeit ohne IDE-Launen

---

## ğŸ“‘ Inhaltsverzeichnis

1. [Problemanalyse](#problemanalyse)
2. [Modelle & Provider](#modelle--provider)
3. [Strikte Projekt-Regeln](#strikte-projekt-regeln)
4. [Auto-Fix-System](#auto-fix-system)
5. [Cache-Strategien](#cache-strategien)
6. [CI/CD Pipeline](#cicd-pipeline)
7. [Desktop-App (Tauri)](#desktop-app-tauri)
8. [Alternativen zu Cursor](#alternativen-zu-cursor)
9. [Checkliste](#checkliste)
10. [Schnellstart](#schnellstart)

---

## Problemanalyse

### Warum passieren Cache-Probleme?

| Problem | Ursache | LÃ¶sung |
|---------|---------|--------|
| **IDE-GedÃ¤chtnisverlust** | Zustand in Tool-Sessions, nicht im Repo | Alles ins Git, keine IDE-Sessions |
| **Cache-Fehler lokal vs. extern** | SW, Browser-Cache, CDN ohne Hash | Hash-Dateinamen, SW-Version bump |
| **Deploy-Illusion** | Lokaler Browser lÃ¤dt alte Dateien | Hard Reload, CDN Purge |
| **Konsole-Verwirrung** | Dev vs. Prod Build unklar | Version-Label im Footer |

### Die LÃ¶sung: Ein-Knopf-Automatisierung

```
[SchlÃ¼ssel drehen] â†’ Clean â†’ Build â†’ Hash â†’ SW â†’ Deploy â†’ Purge
```

---

## Modelle & Provider

### Kostenlose/GÃ¼nstige Optionen

| Provider | Modelle | Kosten | StÃ¤rke |
|----------|---------|--------|--------|
| **Ollama (lokal)** | llama3.1, deepseek-coder, mistral | 0â‚¬ | Offline, keine Datenabfluss |
| **Groq** | LLaMA 3.1, Mixtral | GÃ¼nstig | Sehr schnell |
| **OpenRouter** | DeepSeek, Qwen, Codestral | Pay-per-use | Viele Modelle |
| **Together.ai** | Diverse | Pay-per-use | GÃ¼nstige Inference |

### Empfohlene Konfiguration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AUFGABE              â”‚  MODELL              â”‚  TEMPERATURE     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Autocomplete         â”‚  DeepSeek Coder      â”‚  0.0 - 0.1       â”‚
â”‚  Chat/ErklÃ¤rungen     â”‚  Llama 3.1 70B       â”‚  0.2             â”‚
â”‚  Code-Generierung     â”‚  Codestral/Qwen 2.5  â”‚  0.1             â”‚
â”‚  Refactoring          â”‚  DeepSeek Coder      â”‚  0.0             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ollama Installation

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows: Installer von https://ollama.com/download

# Modelle laden
ollama pull llama3.1
ollama pull deepseek-coder
ollama pull mistral
ollama pull nomic-embed-text  # FÃ¼r Embeddings

# Server starten
ollama serve
```

---

## Strikte Projekt-Regeln

### System-Prompt fÃ¼r AI-Assistenten

```markdown
Du bist ein prÃ¤ziser Coding-Assistent fÃ¼r TogetherSystems.

STRIKTE REGELN:
1. Befolge Anweisungen EXAKT - Ã¤ndere Architektur nur auf explizite Anfrage
2. Verwende NUR Libraries aus dem Repo oder Lockfile
3. Schreibe kompilierbaren, getesteten Code
4. Halte dich an vorhandene Build-Skripte, Makefile und CI
5. Bei Unsicherheit: FRAGE ZUERST
6. Keine eigenmÃ¤chtigen Refactorings
7. Deterministische Ã„nderungen, keine "KreativitÃ¤t"
```

### Temperature-Einstellungen

| Aufgabe | Temperature | BegrÃ¼ndung |
|---------|-------------|------------|
| Code-Completion | 0.0 | Maximale PrÃ¤zision |
| Bugfixes | 0.0 - 0.1 | Keine Experimente |
| Neue Features | 0.1 - 0.2 | Leicht kreativ |
| ErklÃ¤rungen | 0.2 - 0.3 | NatÃ¼rlicher Text |

---

## Auto-Fix-System

### Ein-Knopf-LÃ¶sung

```bash
# AusfÃ¼hren
./scripts/auto_fix.sh

# Oder via Make
make fix
```

### Was passiert?

```
[1/7] Clean          - LÃ¶sche altes Build-Verzeichnis
[2/7] Build          - Deterministischer Build (HTML, MDâ†’HTML)
[3/7] Hash           - Cache-Busting: style.css â†’ style.abc123.css
[4/7] SW Bump        - Service Worker Version: osos-cache-v1733140000
[5/7] Metadata       - Injiziere Build-Info in HTML Footer
[6/7] Deploy         - rsync/git push (optional)
[7/7] CDN Purge      - Cloudflare Cache leeren (optional)
```

### Umgebungs-Switch

```bash
# Development: Kein SW, kein Cache, Live Server
./scripts/start.sh dev

# Staging: Full Build, lokale Preview
./scripts/start.sh staging

# Production: Full Build + Deploy + CDN Purge
./scripts/start.sh prod

# Oder via Make
make dev
make staging
make prod
```

---

## Cache-Strategien

### 1. Asset-Hashing (Cache-Busting)

```
VORHER:  style.css, app.js
NACHHER: style.abc123def.css, app.789xyz012.js
```

Vorteile:
- Browser lÃ¤dt neue Version automatisch
- Immutable caching mÃ¶glich (1 Jahr)
- Keine manuellen Cache-Invalidierungen

### 2. Service Worker

```javascript
const CACHE = 'osos-cache-v1733140000';  // Auto-generated

// Install: Cache minimal
self.addEventListener('install', e => {
    e.waitUntil(caches.open(CACHE).then(c => c.addAll(['/index.html'])));
});

// Activate: LÃ¶sche alte Caches
self.addEventListener('activate', e => {
    e.waitUntil(
        caches.keys().then(keys => 
            Promise.all(keys.filter(k => k !== CACHE).map(k => caches.delete(k)))
        )
    );
});

// Fetch: HTML immer frisch, Assets aus Cache
self.addEventListener('fetch', e => {
    if (e.request.url.includes('.html')) {
        e.respondWith(fetch(e.request, { cache: 'no-store' }));
    } else {
        e.respondWith(caches.match(e.request) || fetch(e.request));
    }
});
```

### 3. HTTP-Header (Server)

```nginx
# Nginx
location ~* \.(css|js|png|svg|woff2)$ {
    add_header Cache-Control "public, max-age=31536000, immutable";
}
location ~* \.(html)$ {
    add_header Cache-Control "no-cache, no-store, must-revalidate";
}
```

### 4. CDN Purge

```bash
# Cloudflare
curl -X POST "https://api.cloudflare.com/client/v4/zones/$ZONE_ID/purge_cache" \
    -H "Authorization: Bearer $API_TOKEN" \
    -d '{"purge_everything":true}'
```

---

## CI/CD Pipeline

### GitHub Actions Workflow

```yaml
name: Build & Deploy
on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: chmod +x scripts/*.sh
      - run: make docs
      - uses: actions/upload-pages-artifact@v3
        with: { path: docs_build }

  deploy:
    needs: build
    runs-on: ubuntu-latest
    permissions: { pages: write, id-token: write }
    steps:
      - uses: actions/deploy-pages@v4
```

### Pre-Commit Hooks

```bash
# .git/hooks/pre-commit
#!/bin/bash
set -e

# Lint Markdown
markdownlint *.md

# Validate Links
./scripts/validate_links.sh

# Build Test
make docs
```

---

## Desktop-App (Tauri)

### Warum Tauri statt Electron?

| Aspekt | Tauri | Electron |
|--------|-------|----------|
| Bundle-GrÃ¶ÃŸe | ~3-10 MB | ~150+ MB |
| RAM-Verbrauch | Niedrig | Hoch |
| Native Feeling | Ja | Nein |
| Auto-Updates | Ja | Ja |

### Quick Start

```bash
# Prerequisites
npm install

# Development
cargo tauri dev

# Build fÃ¼r alle Plattformen (via CI)
cargo tauri build
```

### Release Workflow

```
Tag v1.0.0 â†’ GitHub Action â†’ 
â”œâ”€â”€ Windows: .msi
â”œâ”€â”€ macOS Intel: .dmg
â”œâ”€â”€ macOS ARM: .dmg
â””â”€â”€ Linux: .AppImage
```

---

## Alternativen zu Cursor

### 1. Continue.dev (VSCode/JetBrains)

**Kostenlos, Open Source**

```json
// .continuerc.json
{
    "models": [
        {
            "title": "DeepSeek Coder",
            "provider": "ollama",
            "model": "deepseek-coder"
        },
        {
            "title": "Llama 3.1 (Groq)",
            "provider": "groq",
            "model": "llama-3.1-70b-versatile"
        }
    ]
}
```

### 2. Aider (CLI)

**Git-aware, sehr stabil**

```bash
# Installation
pip install aider-chat

# Starten
aider --model ollama/deepseek-coder

# Mit Konfiguration
aider --config .aider.conf.yml
```

### 3. Codeium (VSCode)

**Kostenlos, gute Autocomplete**

- VSCode Extension installieren
- Kein API-Key nÃ¶tig fÃ¼r Basis-Funktionen

### 4. Windsurf IDE

**GÃ¼nstige Cursor-Alternative**

- Ã„hnliche Features
- Kontrollierbare Agents
- GÃ¼nstiger als Cursor Pro

### Strategie-Empfehlung

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOKAL (0â‚¬)           â†’ Ollama + Continue.dev + Aider          â”‚
â”‚  CLOUD (bei Bedarf)   â†’ Groq/OpenRouter (nur wenn nÃ¶tig)       â”‚
â”‚  BACKUP              â†’ Alle Prompts/Settings im Git            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Checkliste

### âœ… Gleiche Ansicht Ã¼berall

- [ ] **Service Worker**: Version bump bei Release
- [ ] **Asset-Hashing**: CSS/JS mit Hash im Namen
- [ ] **CDN Purge**: Nach Deploy global leeren
- [ ] **HTML no-cache**: Browser holt immer frisch
- [ ] **Footer-Label**: Build/Commit/Cache sichtbar
- [ ] **Repo-Wahrheit**: Alle Settings im Git
- [ ] **Temperature niedrig**: 0.0 - 0.2

### âœ… Stabile Entwicklung

- [ ] **Make-Targets**: `dev`, `staging`, `prod`
- [ ] **Auto-Fix-Script**: Ein Knopf fÃ¼r alles
- [ ] **Pre-Commit Hooks**: Lint + Validate
- [ ] **CI/CD**: Automatische Builds
- [ ] **Strikte Prompts**: AI folgt Regeln

### âœ… Cross-Platform

- [ ] **Tauri Config**: `src-tauri/tauri.conf.json`
- [ ] **Release Workflow**: `.github/workflows/release-desktop.yml`
- [ ] **Checksums**: SHA256 fÃ¼r alle Artifacts

---

## Schnellstart

### 1. Repository klonen

```bash
git clone https://github.com/Myopenai/togethersystems.git
cd togethersystems
```

### 2. Ollama installieren (optional, fÃ¼r lokale AI)

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull deepseek-coder
ollama pull llama3.1
```

### 3. Development starten

```bash
# Scripts ausfÃ¼hrbar machen
chmod +x scripts/*.sh

# Dev-Server starten
make dev
# oder: ./scripts/start.sh dev
```

### 4. Production Deploy

```bash
make prod
# oder: ./scripts/start.sh prod
```

### 5. Aider fÃ¼r Coding

```bash
# Mit lokalem Modell
make aider-local

# Mit Groq
GROQ_API_KEY=... make aider-groq
```

---

## Dateien-Ãœbersicht

```
togethersystems/
â”œâ”€â”€ .continuerc.json           # Continue.dev Konfiguration
â”œâ”€â”€ .aider.conf.yml            # Aider Konfiguration
â”œâ”€â”€ Makefile                   # Build-Targets
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ auto_fix.sh            # Ein-Knopf-Automatisierung
â”‚   â”œâ”€â”€ start.sh               # Umgebungs-Switch
â”‚   â”œâ”€â”€ build_docs.sh          # MDâ†’HTML Pipeline
â”‚   â””â”€â”€ aider-start.sh         # Aider mit Tests
â”œâ”€â”€ src-tauri/
â”‚   â”œâ”€â”€ tauri.conf.json        # Tauri Konfiguration
â”‚   â”œâ”€â”€ Cargo.toml             # Rust Dependencies
â”‚   â””â”€â”€ src/main.rs            # Tauri Main
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ build.yml              # CI/CD Pipeline
â”‚   â””â”€â”€ release-desktop.yml    # Desktop-App Releases
â””â”€â”€ docs_build/                # Generated (nicht einchecken)
```

---

## Support

Bei Problemen:

1. **Issue erstellen**: https://github.com/Myopenai/togethersystems/issues
2. **Logs prÃ¼fen**: Browser Console, Terminal Output
3. **Cache leeren**: Hard Reload (Ctrl+Shift+R)

---

**[.TTT T,.&T,,.T,,,.T.] TOGETHERSYSTEMS. INTERNATIONAL TTT**

*Â© 2025 Raymond Demitrio Tel*
